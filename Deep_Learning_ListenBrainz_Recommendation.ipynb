{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning ListenBrainz Recommendation\n",
    "Written by Dean Cochran\n",
    "\n",
    "# Abstract\n",
    "The objective of this notebook is to generate a working example of how it is possible to improve recommendation of songs to users by incorporating more contextual information of the listening session. These contextual pieces of information can include, the time features of the listen, the audio features from the song listened to, and the metadata from the song listened to.\n",
    "\n",
    "## User Note:\n",
    "The requirements of this notebook are listed in the requirements.txt file within the project directory. Additionally, the same dataset from the listenbrainz open source data dumps must be downloaded to compute the notebook. \n",
    "\n",
    "Warning: This is a 39 GB data file with a massive download time, and this file takes around 45min to run with my 2018 Monterey macOS\n",
    "\n",
    "I recommend preparing your computer for optimal download speeds. I utilize the ListenBrainz full export in the directory: /listenbrainz/fullexport/listenbrainz-dump-789-20220315-040002-fulllistenbrainz-listens-dump-789-20220315-040002-full.tar\n",
    "\n",
    "Once downloaded, unzip the file within the project directory and your all set to run the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorflow-recommenders in /usr/local/lib/python3.9/site-packages (0.6.0)\n",
      "Requirement already satisfied: tensorflow>=2.6.0 in /usr/local/lib/python3.9/site-packages (from tensorflow-recommenders) (2.8.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.9/site-packages (from tensorflow-recommenders) (1.0.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.9/site-packages (from absl-py>=0.1.6->tensorflow-recommenders) (1.16.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders) (1.6.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders) (3.19.1)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders) (1.43.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders) (2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders) (3.6.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders) (1.1.2)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders) (2.8.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders) (3.3.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders) (2.8.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders) (13.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders) (60.5.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders) (1.22.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders) (4.0.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders) (0.23.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow-recommenders) (1.13.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow>=2.6.0->tensorflow-recommenders) (0.37.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders) (2.3.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders) (2.0.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders) (2.27.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders) (3.3.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders) (4.10.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders) (2.0.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders) (1.26.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow-recommenders) (3.1.1)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-recommenders\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "from tensorflow import keras  \n",
    "from typing import Dict, Text\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aggreation\n",
    "\n",
    "Utilizing the pre-made function in the project folder, we can aggregate and process our information in little to no time at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to false if you have the 'listenbrainz-listens-dump-fullexport.csv' in the project directory\n",
    "PROCESS_LISTENBRAINZ_DUMP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate all the python info\n",
    "if PROCESS_LISTENBRAINZ_DUMP:\n",
    "    !python3 process_lb_dump.py\n",
    "\n",
    "# read the listenbrainz csv we just made\n",
    "df = pd.read_csv('listenbrainz-listens-dump-fullexport.csv').drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "Our text features in the data are not the cleanest but they will get the job done! So we should make take the following precautions and preform the following actions:\n",
    "\n",
    "- remove users that have under 10 listens\n",
    "- make sure all user_ids are string\n",
    "- make sure all timestamps are int\n",
    "- make sure all recording_msids are string\n",
    "- make sure all track_names are string\n",
    "- make sure all artist_names are string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data with this chunck of code\n",
    "removeUsers = [i for i,val in zip(df['user_id'].value_counts().index,df['user_id'].value_counts()) if val < 10]\n",
    "df[df['user_id'].apply(lambda x: x not in removeUsers)]\n",
    "df['user_id'] = df['user_id'].apply(lambda x: str(x))\n",
    "df['timestamp'] = df['timestamp'].apply(lambda x: int(x))\n",
    "df['track_name'] = df['track_name'].apply(lambda x: str(x))\n",
    "df['artist_name'] = df['artist_name'].apply(lambda x: str(x))\n",
    "df['recording_msid'] = df['recording_msid'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations from our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows 2534176\n",
      "# cols 5\n",
      "avg # of listens per song:  4.88714638628917\n",
      "The Beatles         21657\n",
      "Radiohead           14986\n",
      "Lil' Wayne          13786\n",
      "Linkin Park         13401\n",
      "System of a Down    11353\n",
      "Green Day           10595\n",
      "Pink Floyd          10024\n",
      "Metallica            9680\n",
      "Nine Inch Nails      8893\n",
      "Depeche Mode         8835\n",
      "Name: artist_name, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>track_name</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>recording_msid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15934</td>\n",
       "      <td>0</td>\n",
       "      <td>Calvi</td>\n",
       "      <td>Izia</td>\n",
       "      <td>63887ddb-d3b0-4f5f-b4ff-fb7caf072538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15934</td>\n",
       "      <td>0</td>\n",
       "      <td>Sous les pavés</td>\n",
       "      <td>Izia</td>\n",
       "      <td>365a4bc1-c8b5-4173-850b-394662642599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15934</td>\n",
       "      <td>0</td>\n",
       "      <td>Trop vite</td>\n",
       "      <td>Izia</td>\n",
       "      <td>b8cd1c25-b7c4-4f0b-b0fd-2d381ee0d583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15934</td>\n",
       "      <td>0</td>\n",
       "      <td>Sunset</td>\n",
       "      <td>Izia</td>\n",
       "      <td>c9a8fde1-0703-4ccd-9e69-245b9b323ae9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15934</td>\n",
       "      <td>0</td>\n",
       "      <td>Esseulés</td>\n",
       "      <td>Izia</td>\n",
       "      <td>8c279298-c4aa-4eb7-a6f6-1d538b8f11d6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534171</th>\n",
       "      <td>11325</td>\n",
       "      <td>1128106599</td>\n",
       "      <td>I Know Why</td>\n",
       "      <td>Sheryl Crow</td>\n",
       "      <td>d7bf3eae-fc71-459b-8d31-3e25a8660bd0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534172</th>\n",
       "      <td>12612</td>\n",
       "      <td>1128106599</td>\n",
       "      <td>The National Anthem</td>\n",
       "      <td>Radiohead</td>\n",
       "      <td>0267dfbc-f93e-4bbc-a063-b365b70443ce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534173</th>\n",
       "      <td>14629</td>\n",
       "      <td>1128106605</td>\n",
       "      <td>Iron Man</td>\n",
       "      <td>Black Sabbath</td>\n",
       "      <td>ed49724a-7660-4822-856c-408f82ab17ec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534174</th>\n",
       "      <td>12900</td>\n",
       "      <td>1128106608</td>\n",
       "      <td>The Irony of It All</td>\n",
       "      <td>The Streets</td>\n",
       "      <td>0343337f-54dd-40b5-83a6-7be14e22d411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534175</th>\n",
       "      <td>15185</td>\n",
       "      <td>1128106635</td>\n",
       "      <td>Something</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>99408df0-30e4-4c7a-904e-96e2064e2115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2534176 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id   timestamp           track_name    artist_name  \\\n",
       "0         15934           0                Calvi           Izia   \n",
       "1         15934           0       Sous les pavés           Izia   \n",
       "2         15934           0            Trop vite           Izia   \n",
       "3         15934           0               Sunset           Izia   \n",
       "4         15934           0             Esseulés           Izia   \n",
       "...         ...         ...                  ...            ...   \n",
       "2534171   11325  1128106599           I Know Why    Sheryl Crow   \n",
       "2534172   12612  1128106599  The National Anthem      Radiohead   \n",
       "2534173   14629  1128106605             Iron Man  Black Sabbath   \n",
       "2534174   12900  1128106608  The Irony of It All    The Streets   \n",
       "2534175   15185  1128106635            Something    The Beatles   \n",
       "\n",
       "                               recording_msid  \n",
       "0        63887ddb-d3b0-4f5f-b4ff-fb7caf072538  \n",
       "1        365a4bc1-c8b5-4173-850b-394662642599  \n",
       "2        b8cd1c25-b7c4-4f0b-b0fd-2d381ee0d583  \n",
       "3        c9a8fde1-0703-4ccd-9e69-245b9b323ae9  \n",
       "4        8c279298-c4aa-4eb7-a6f6-1d538b8f11d6  \n",
       "...                                       ...  \n",
       "2534171  d7bf3eae-fc71-459b-8d31-3e25a8660bd0  \n",
       "2534172  0267dfbc-f93e-4bbc-a063-b365b70443ce  \n",
       "2534173  ed49724a-7660-4822-856c-408f82ab17ec  \n",
       "2534174  0343337f-54dd-40b5-83a6-7be14e22d411  \n",
       "2534175  99408df0-30e4-4c7a-904e-96e2064e2115  \n",
       "\n",
       "[2534176 rows x 5 columns]"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows,cols = df.shape\n",
    "print('# rows',rows)\n",
    "print('# cols',cols)\n",
    "print('avg # of listens per song: ',np.mean(df['recording_msid'].value_counts()))\n",
    "print(df['artist_name'].value_counts()[:10])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13277    13526\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# who listens to Lil' Wayne the most\n",
    "df[df['artist_name']==\"Lil' Wayne\"]['user_id'].value_counts()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14512    4098\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# who listens to The Beatles the most\n",
    "df[df['artist_name']==\"The Beatles\"]['user_id'].value_counts()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15748    2155\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# who listens to Radiohead the most\n",
    "df[df['artist_name']==\"Radiohead\"]['user_id'].value_counts()[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up our tensor datasets\n",
    "\n",
    "we need two datasets:\n",
    "\n",
    "- one to represent every listen\n",
    "- one to represent every unique recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/42/c20b9csx1tv016cm04x9syc40000gn/T/ipykernel_49586/823632245.py:10: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  recordingdataset = tf.data.Dataset.from_tensor_slices(dict(df.groupby('recording_msid')['artist_name','track_name'].first().reset_index()))\n"
     ]
    }
   ],
   "source": [
    "# using tensorflow to create TF datsets of the listening behaviour and list of unique recordings\n",
    "dataset = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "listens = dataset.map(lambda x: {\n",
    "    \"recording_msid\":x['recording_msid'],\n",
    "    \"user_id\": x['user_id'],\n",
    "    \"timestamp\": x['timestamp'],\n",
    "    \"artist_name\": x['artist_name'],\n",
    "})\n",
    "\n",
    "recordingdataset = tf.data.Dataset.from_tensor_slices(dict(df.groupby('recording_msid')['artist_name','track_name'].first().reset_index()))\n",
    "recording_msids = recordingdataset.map(lambda x: {\n",
    "    \"recording_msid\":x['recording_msid'],\n",
    "    \"artist_name\": x['artist_name'],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating unique lists of users, recordings, and our other features \n",
    "This is necessary to create a vocabulary for the user_ids, recording_msids, artist_names, and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'00002d3e-542a-4dca-9ae8-e25e9c0a3781',\n",
       "       b'000034a9-b580-4bf3-b8db-4ccfab32d1a7',\n",
       "       b'00008997-e554-44b3-bcf4-77dccf8f0c0d',\n",
       "       b'00009800-a061-47ae-b5f0-e35eeea7ecee',\n",
       "       b'0000a061-36cf-4e3e-b42b-efab7c4b6846',\n",
       "       b'0000b64d-522e-4e1b-a94b-7a756e4948f6',\n",
       "       b'0000d0ee-a1cc-4983-8a15-d7c0bcf6578b',\n",
       "       b'0000de43-70af-4ad2-bd43-d8250e23e91c',\n",
       "       b'0000e153-ba17-41a3-90b7-0d8d57347530',\n",
       "       b'0000e793-64d7-4aab-aa63-b3fa41adbddd'], dtype=object)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_user_ids = np.unique(np.concatenate(list(recording_msids.batch(1000).map(lambda x: x[\"recording_msid\"]))))\n",
    "unique_user_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'10005', b'10006', b'10012', b'10315', b'1039', b'1049', b'10610',\n",
       "       b'10635', b'10787', b'11'], dtype=object)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_user_ids = np.unique(np.concatenate(list(listens.batch(1000).map(lambda x: x[\"user_id\"]))))\n",
    "unique_user_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'!!!', b'!((0rkza1', b'!Action Pact!', b'!distain', b'\"Demons\"',\n",
       "       b'\"Dusty Rhodes\" Rowe', b'\"Weird Al\" Yankovic',\n",
       "       b'\"the band (aint got a name yet)\"', b'#18',\n",
       "       b'$ bboy muzak (jason levis'], dtype=object)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_artist_names = np.unique(np.concatenate(list(recording_msids.batch(1000).map(lambda x: x[\"artist_name\"]))))\n",
    "unique_artist_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([       0.        ,  1129235.87087087,  2258471.74174174,\n",
       "        3387707.61261261,  4516943.48348348,  5646179.35435435,\n",
       "        6775415.22522523,  7904651.0960961 ,  9033886.96696697,\n",
       "       10163122.83783784])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamps = np.concatenate(list(listens.map(lambda x: x[\"timestamp\"]).batch(100)))\n",
    "\n",
    "max_timestamp = timestamps.max()\n",
    "min_timestamp = timestamps.min()\n",
    "\n",
    "timestamp_buckets = np.linspace(\n",
    "    min_timestamp, max_timestamp, num=1000,\n",
    ")\n",
    "timestamp_buckets[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building User and Recording Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecordingModel(tf.keras.Model):\n",
    "  \"\"\"\n",
    "  This class represents a single recording\n",
    "  The functionality of the model creates an embedding that can describe a recording when passed a dicitonary of a recording_msid and the artist_name\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    max_tokens = 10_000\n",
    "    \n",
    "    ## MSID embeddings\n",
    "    self.msid_embedding = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(vocabulary=unique_recording_msids,mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(unique_recording_msids) + 1, 32)\n",
    "    ])\n",
    "\n",
    "    ## artist names embeddings\n",
    "    self.artist_name_embedding = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(vocabulary=unique_artist_names,mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(unique_artist_names) + 1, 32)\n",
    "    ])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Take the input dictionary, pass it through each input layer,\n",
    "    # and concatenate the result.\n",
    "    return tf.concat([\n",
    "        self.msid_embedding(inputs['recording_msid']),\n",
    "        self.artist_name_embedding(inputs['artist_name'])\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "  \"\"\"\n",
    "  This class represents a single user\n",
    "  The functionality of the model creates an embedding that can describe a user when passed a dicitonary of a user_id and the timestamp of a listen\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    ## user id embeddings\n",
    "    self.user_embedding = tf.keras.Sequential([\n",
    "        tf.keras.layers.StringLookup(vocabulary=unique_user_ids, mask_token=None),\n",
    "        tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),\n",
    "    ])\n",
    "    ## timestamp embeddings\n",
    "    self.timestamp_embedding = tf.keras.Sequential([\n",
    "        tf.keras.layers.Discretization(timestamp_buckets.tolist()),\n",
    "        tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32),\n",
    "    ])\n",
    "    self.normalized_timestamp = tf.keras.layers.Normalization(\n",
    "        axis=None\n",
    "    )\n",
    "    self.normalized_timestamp.adapt(timestamps)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Take the input dictionary, pass it through each input layer,\n",
    "    # and concatenate the result.\n",
    "    return tf.concat([\n",
    "        self.user_embedding(inputs[\"user_id\"]),\n",
    "        self.timestamp_embedding(inputs[\"timestamp\"]),\n",
    "        tf.reshape(self.normalized_timestamp(inputs[\"timestamp\"]), (-1, 1)),\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Query and Candidate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryModel(tf.keras.Model):\n",
    "  \"\"\"Model for encoding user queries.\"\"\"\n",
    "\n",
    "  def __init__(self, layer_sizes):\n",
    "    \"\"\"Model for encoding user queries.\n",
    "\n",
    "    Args:\n",
    "      layer_sizes:\n",
    "        A list of integers where the i-th entry represents the number of units\n",
    "        the i-th layer contains.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    # User model for generating embeddings.\n",
    "    self.embedding_model = UserModel()\n",
    "\n",
    "    # Then construct the layers.\n",
    "    self.dense_layers = tf.keras.Sequential()\n",
    "\n",
    "    # Use the ReLU activation for all but the last layer.\n",
    "    for layer_size in layer_sizes[:-1]:\n",
    "      self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
    "\n",
    "    # No activation for the last layer.\n",
    "    for layer_size in layer_sizes[-1:]:\n",
    "      self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    feature_embedding = self.embedding_model(inputs)\n",
    "    return self.dense_layers(feature_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CandidateModel(tf.keras.Model):\n",
    "  \"\"\"Model for encoding recordings.\"\"\"\n",
    "\n",
    "  def __init__(self, layer_sizes):\n",
    "    \"\"\"Model for encoding recordings.\n",
    "\n",
    "    Args:\n",
    "      layer_sizes:\n",
    "        A list of integers where the i-th entry represents the number of units\n",
    "        the i-th layer contains.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    # Recording model for generating embeddings.\n",
    "    self.embedding_model = RecordingModel()\n",
    "\n",
    "    # Then construct the layers.\n",
    "    self.dense_layers = tf.keras.Sequential()\n",
    "   \n",
    "    # Use the ReLU activation for all but the last layer.\n",
    "    for layer_size in layer_sizes[:-1]:\n",
    "      self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
    "\n",
    "    # No activation for the last layer.\n",
    "    for layer_size in layer_sizes[-1:]:\n",
    "      self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    feature_embedding = self.embedding_model(inputs)\n",
    "    return self.dense_layers(feature_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building ListenBrainz Recommendation Model\n",
    "\n",
    "With both QueryModel and CandidateModel defined, I can put together a combined model and implement our loss and metrics logic. To make things simple, I'll enforce that the model structure is the same across the query and candidate models.\n",
    "\n",
    "\n",
    "Since the expressive power of deep linear models is no greater than that of shallow linear models, we use ReLU activations for all but the last hidden layer. The final hidden layer does not use any activation function: using an activation function would limit the output space of the final embeddings and might negatively impact the performance of the model. For instance, if ReLUs are used in the projection layer, all components in the output embedding would be non-negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListenBrainzModel(tfrs.Model):\n",
    "  # We derive from a custom base class to help reduce boilerplate. Under the hood,\n",
    "  # these are still plain Keras Models.\n",
    "\n",
    "  def __init__(self, layer_sizes):\n",
    "    super().__init__()\n",
    "    self.query_model = QueryModel(layer_sizes)\n",
    "    self.candidate_model = CandidateModel(layer_sizes)\n",
    "    self.task = tfrs.tasks.Retrieval(\n",
    "        metrics = tfrs.metrics.FactorizedTopK(candidates=recording_msids.batch(128).map(self.candidate_model)),\n",
    "    )\n",
    "\n",
    "  def compute_loss(self, features, training=False):\n",
    "    query_embeddings = self.query_model({\n",
    "        \"user_id\": features[\"user_id\"],\n",
    "        \"timestamp\": features[\"timestamp\"],\n",
    "    })\n",
    "    recording_embeddings = self.candidate_model({\n",
    "        \"recording_msid\": features[\"recording_msid\"],\n",
    "        \"artist_name\": features[\"artist_name\"]\n",
    "    })\n",
    "\n",
    "    return self.task(\n",
    "        query_embeddings, recording_embeddings, compute_metrics=not training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data\n",
    "\n",
    "Here we split our data in an 80/20 split and cache the data to fit within our tensorflow recommendation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = listens.shuffle(20_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(20_000)\n",
    "test = shuffled.skip(20_000).take(5_000)\n",
    "\n",
    "cached_train = train.shuffle(20_000).batch(2048)\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit/Eval with different layers \n",
    "\n",
    "Defining deeper models will require us to stack mode layers on top of this first input. A progressively narrower stack of layers, separated by an activation function, is a common pattern:\n",
    "\n",
    "                            +----------------------+\n",
    "                            |      128 x 64        |\n",
    "                            +----------------------+\n",
    "                                       | relu\n",
    "                          +--------------------------+\n",
    "                          |        256 x 128         |\n",
    "                          +--------------------------+\n",
    "                                       | relu\n",
    "                        +------------------------------+\n",
    "                        |          ... x 256           |\n",
    "                        +------------------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "# specifying how many times we will run our model metrics for the visualizations (i.e. val_freq = 20 means we have 20 points for each model to plot)\n",
    "val_freq = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling\n",
      "fitting\n"
     ]
    }
   ],
   "source": [
    "# SINGLE LAYER MODEL\n",
    "print('SINGLE LAYER MODEL')\n",
    "model = ListenBrainzModel([32])\n",
    "print('compiling')\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "print('fitting')\n",
    "one_layer_history = model.fit(\n",
    "    cached_train,\n",
    "    validation_data=cached_test,\n",
    "    validation_freq=val_freq,\n",
    "    epochs=num_epochs,\n",
    "    verbose=0)\n",
    "\n",
    "# TWO LAYER MODEL\n",
    "print('TWO LAYER MODEL')\n",
    "model = ListenBrainzModel([64, 32])\n",
    "print('compiling')\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "print('fitting')\n",
    "two_layer_history = model.fit(\n",
    "    cached_train,\n",
    "    validation_data=cached_test,\n",
    "    validation_freq=val_freq,\n",
    "    epochs=num_epochs,\n",
    "    verbose=0)\n",
    "\n",
    "# THREE LAYER MODEL\n",
    "print('THREE LAYER MODEL')\n",
    "model = ListenBrainzModel([128, 64, 32])\n",
    "print('compiling')\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "print('fitting')\n",
    "three_layer_history = model.fit(\n",
    "    cached_train,\n",
    "    validation_data=cached_test,\n",
    "    validation_freq=val_freq,\n",
    "    epochs=num_epochs,\n",
    "    verbose=0)\n",
    "\n",
    "\n",
    "# Computing Accuracy Visualization\n",
    "num_validation_runs = len(one_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"])\n",
    "epochs = [(x + 1)* 5 for x in range(num_validation_runs)]\n",
    "\n",
    "#plot every models results\n",
    "plt.plot(epochs, one_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"], label=\"1 layer\")\n",
    "plt.plot(epochs, two_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"], label=\"2 layers\")\n",
    "plt.plot(epochs, three_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"], label=\"3 layers\")\n",
    "plt.title(\"Accuracy vs epoch\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Top-100 accuracy\");\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using best model to serve predictions\n",
    "\n",
    "Since the single layer seemed to preform more optimally, we can take the computed model and utilize it to generate recommendations without any ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling\n",
      "fitting\n",
      "evaluating\n",
      "2/2 [==============================] - 76s 17s/step - factorized_top_k/top_1_categorical_accuracy: 0.0184 - factorized_top_k/top_5_categorical_accuracy: 0.1020 - factorized_top_k/top_10_categorical_accuracy: 0.1906 - factorized_top_k/top_50_categorical_accuracy: 0.4728 - factorized_top_k/top_100_categorical_accuracy: 0.5290 - loss: 14431.0150 - regularization_loss: 0.0000e+00 - total_loss: 14431.0150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.018400000408291817,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.10199999809265137,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.19059999287128448,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.47279998660087585,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.5289999842643738,\n",
       " 'loss': 5879.7705078125,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 5879.7705078125}"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-Initializng Best Model, change layers in ListenBrainzModel if you want to see the predictions of a different model\n",
    "num_epochs = 100\n",
    "val_freq = 20\n",
    "\n",
    "# # # # # # # # # # # # # # \n",
    "model = ListenBrainzModel([32])\n",
    "# # # # # # # # # # # # # #\n",
    "\n",
    "print('compiling')\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "print('fitting')\n",
    "one_layer_history = model.fit(\n",
    "    cached_train,\n",
    "    epochs=num_epochs,\n",
    "    verbose=0)\n",
    "print('evaluating')\n",
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x191c61910>"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a model that takes in raw query features and recommends recordings out of the entire recordings dataset.\n",
    "# Replace key name with what kind of predicitons you would like to recieve\n",
    "# ex.recording_msids.batch(100).map(lambda x: (x['recording_msid'], model.candidate_model(x))) # give's recording_msid recommendations\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.query_model)\n",
    "index.index_from_dataset(\n",
    "    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "    recording_msids.batch(100).map(lambda x: (x['artist_name'], model.candidate_model(x))) \n",
    "    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(df, user_id, row_num):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    Given an instance in our data, we can give recommendations for a user at a particular timestamp\n",
    "\n",
    "    params:\n",
    "    df - pd.DataFrame\n",
    "    user_id - Integer corresponding to a user id in the data frame\n",
    "    row_num - Integer corresponding to which instance of the users listens you would like to use (UN-TESTED, most likely un-important and gives identical results)\n",
    "\n",
    "    \"\"\"\n",
    "    return df[df['user_id'] == f'{user_id}'].iloc[row_num][0:2].map(lambda x: tf.expand_dims(x, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LILWAYNE FAN BOY: User 13277 listens to:\n",
      "Lil' Wayne     13526\n",
      "Drake           5749\n",
      "Bright Eyes     3325\n",
      "Kanye West      3055\n",
      "The Format      2698\n",
      "Name: artist_name, dtype: int64\n",
      "\n",
      "Artist name of each recording id for user 13277:\n",
      "[[b'James Binney' b\"Juelz Santana feat. Young Jeezy & Lil' Wayne\"\n",
      "  b'Drake' b'James Binney' b'Weezer' b\"Lil' Wayne\" b\"Lil' Wayne\"\n",
      "  b\"Lil' Wayne\" b'Peter Frampton' b'Bright Eyes']]\n"
     ]
    }
   ],
   "source": [
    "# Get recommendations.\n",
    "query = get_query(df,13277,15)\n",
    "_, artists = index(query)\n",
    "print('LILWAYNE FAN BOY: User 13277 listens to:')\n",
    "print(# what other artists does he listen to\n",
    "df[df['user_id']=='13277']['artist_name'].value_counts()[:5])\n",
    "print()\n",
    "print(f\"Artist name of each recording id for user 13277:\")\n",
    "print(artists.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE BEATLES FAN BOY: User 14512 listens to:\n",
      "The Beatles      4098\n",
      "Led Zeppelin     1078\n",
      "Queen             963\n",
      "Guns N' Roses     836\n",
      "Graham Nash       791\n",
      "Name: artist_name, dtype: int64\n",
      "\n",
      "Artist name of each recording idfor user 14512:\n",
      "[[b'Megadeth' b'The Rolling Stones' b'Skid Row' b'The Who' b'The Who'\n",
      "  b'The Who' b'The Who' b'The Who' b'Rush' b'Van Halen']]\n"
     ]
    }
   ],
   "source": [
    "# Get recommendations.\n",
    "query = get_query(df,14512,15)\n",
    "_, artists = index(query)\n",
    "print('THE BEATLES FAN BOY: User 14512 listens to:')\n",
    "print(# what other artists does he listen to\n",
    "df[df['user_id']=='14512']['artist_name'].value_counts()[:5])\n",
    "print()\n",
    "print(f\"Artist name of each recording idfor user 14512:\")\n",
    "print(artists.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RADIOHEAD FAN BOY: User 15748 listens to:\n",
      "Nine Inch Nails          2995\n",
      "Radiohead                2155\n",
      "The Beatles              1496\n",
      "The Smashing Pumpkins     861\n",
      "Coldplay                  649\n",
      "Name: artist_name, dtype: int64\n",
      "Artist name of each recording idfor user 15748:\n",
      "[[b'Blue October' b'The Smashing Pumpkins' b'Weezer' b'A Perfect Circle'\n",
      "  b'Gwen Stefani' b'Queens of the Stone Age' b'Feist' b'Radiohead'\n",
      "  b'Radiohead' b'Radiohead']]\n"
     ]
    }
   ],
   "source": [
    "# Get recommendations.\n",
    "query = get_query(df,15748,15)\n",
    "_, artists = index(query)\n",
    "print('RADIOHEAD FAN BOY: User 15748 listens to:')\n",
    "print(# what other artists does he listen to\n",
    "df[df['user_id']=='15748']['artist_name'].value_counts()[:5])\n",
    "print(f\"Artist name of each recording idfor user 15748:\")\n",
    "print(artists.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1987.0479259490967 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
